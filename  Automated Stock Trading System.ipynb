{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "so3ost9YeJBY",
      "metadata": {
        "id": "so3ost9YeJBY"
      },
      "source": [
        " # Automated Stock Trading System"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd479d7c",
      "metadata": {},
      "source": [
        "### 1. Value Iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0vm3I9G8NI7h",
      "metadata": {
        "id": "0vm3I9G8NI7h"
      },
      "source": [
        "\n",
        "\n",
        "#### 1(a). State Value Computation using Bellman Optimality Equation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 653,
      "id": "ZUCQkxadEUID",
      "metadata": {
        "id": "ZUCQkxadEUID"
      },
      "outputs": [],
      "source": [
        "def compute_state_value(state, V, transition_probs, rewards, actions, gamma, states):\n",
        "    \"\"\"\n",
        "    Computes the value of a state using the Bellman optimality equation.\n",
        "\n",
        "    Parameters:\n",
        "    - state: Current state.\n",
        "    - V: Current value function estimate.\n",
        "    - transition_probs: Transition model of the environment (P(s' | s, a)).\n",
        "    - rewards: Reward function (R(s, a, s')).\n",
        "    - actions: Actions available in the state.\n",
        "    - gamma: Discount factor for future rewards.\n",
        "    - states: List of all states.\n",
        "\n",
        "    Returns:\n",
        "    - Value of the state (V*(s)).\n",
        "    \"\"\"\n",
        "\n",
        "    # List to store expected values for each action.\n",
        "    expected_values = []\n",
        "\n",
        "    # Iterate over each action available in the current state.\n",
        "    for action in actions[state]:\n",
        "\n",
        "        # Initialize the expected value for this action (corresponding to the summation term in the Bellman equation).\n",
        "        action_value = 0\n",
        "\n",
        "        # Calculate the expected value for the action by summing over all possible next states.\n",
        "        for next_state in states:\n",
        "\n",
        "            # Transition probability from 'state' to 'next_state' given 'action' (P(s' | s, a)).\n",
        "            transition_prob = transition_probs[state][action][next_state]\n",
        "\n",
        "            # Immediate reward for taking 'action' in 'state' and ending up in 'next_state'\n",
        "            immediate_reward = rewards[state][action][next_state]\n",
        "\n",
        "            # Update the action's expected value using the Bellman equation components.\n",
        "            action_value += transition_prob * (immediate_reward + gamma * V[next_state])\n",
        "\n",
        "        # Append the expected value of the action to the list.\n",
        "        expected_values.append(action_value)\n",
        "\n",
        "    # Return the highest expected value among all actions, which corresponds to the max_a operation in the Bellman equation.\n",
        "    return max(expected_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mWizkgEONNdu",
      "metadata": {
        "id": "mWizkgEONNdu"
      },
      "source": [
        "\n",
        "\n",
        "#### 1(b). Policy Extraction based on Value Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 654,
      "id": "N8KvU29xJQfu",
      "metadata": {
        "id": "N8KvU29xJQfu"
      },
      "outputs": [],
      "source": [
        "def extract_policy(V, transition_probs, rewards, actions, gamma, states):\n",
        "    \"\"\"\n",
        "    Extracts the optimal policy based on a given value function using the Bellman optimality equation.\n",
        "\n",
        "    Parameters:\n",
        "    - V: Value function based on which policy is extracted.\n",
        "    - transition_probs: Transition model of the environment (P(s' | s, a)).\n",
        "    - rewards: Reward function (R(s, a, s')).\n",
        "    - actions: Dictionary of available actions for each state.\n",
        "    - gamma: Discount factor for future rewards.\n",
        "    - states: List of all states.\n",
        "\n",
        "    Returns:\n",
        "    - Optimal policy (a mapping from states to optimal actions).\n",
        "    \"\"\"\n",
        "\n",
        "    policy = {}\n",
        "\n",
        "    # Construct the policy by iterating over each state.\n",
        "    for state in states:\n",
        "\n",
        "        best_action_value = float('-inf')\n",
        "        best_action = None\n",
        "\n",
        "        # Evaluate each possible action to find the best one for this state\n",
        "        for action in actions[state]:\n",
        "\n",
        "            action_value = 0  # Initialize the expected value for this action\n",
        "\n",
        "            # Compute the expected value for the state using this action\n",
        "            for next_state in states:\n",
        "                transition_prob = transition_probs[state][action][next_state]\n",
        "\n",
        "                # Immediate reward for taking 'action' in 'state'.\n",
        "                immediate_reward = rewards[state][action][next_state]\n",
        "\n",
        "                # Update the action's value using components of the Bellman equation\n",
        "                action_value += transition_prob * (immediate_reward + gamma * V[next_state])\n",
        "\n",
        "            # Update the best action if this action's value is higher\n",
        "            if action_value > best_action_value:\n",
        "                best_action_value = action_value\n",
        "                best_action = action\n",
        "\n",
        "        # Update the policy for the current state with the best action\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4EEPh2dxObXW",
      "metadata": {
        "id": "4EEPh2dxObXW"
      },
      "source": [
        "#### 1(c). Value Iteration Algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 655,
      "id": "13GUY03tJK2l",
      "metadata": {
        "id": "13GUY03tJK2l"
      },
      "outputs": [],
      "source": [
        "def value_iteration(states, actions, transition_probs, rewards, gamma=0.9, theta=1e-3, debug_print=False):\n",
        "    \"\"\"\n",
        "    Implements the value iteration algorithm to find the optimal policy and value function.\n",
        "\n",
        "    Parameters:\n",
        "    - states: List of states.\n",
        "    - actions: Dictionary of actions.\n",
        "    - transition_probs: Transition probabilities in a 3-D dictionary format.\n",
        "    - rewards: Reward dictionary.\n",
        "    - gamma: Discount factor for future rewards.\n",
        "    - theta: Convergence threshold. Value iteration stops when the change in value function is smaller than eps.\n",
        "\n",
        "    Returns:\n",
        "    - V: Optimal value function.\n",
        "    - policy: Optimal policy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization\n",
        "    V = {state: 0 for state in states}\n",
        "    iteration = 0\n",
        "\n",
        "    while True:\n",
        "        new_V = V.copy()\n",
        "        delta = 0\n",
        "\n",
        "        # Value Update\n",
        "        for state in states:\n",
        "            new_V[state] = compute_state_value(state, V, transition_probs, rewards, actions, gamma, states)\n",
        "            delta = max(delta, abs(new_V[state] - V[state]))\n",
        "\n",
        "        # Optional: Print debug information\n",
        "        if debug_print:\n",
        "            iteration += 1\n",
        "            print(f\"Value Iteration {iteration}: Max Change: {delta}, Value Function: {new_V}\")\n",
        "\n",
        "        # Convergence Check\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "        V = new_V\n",
        "\n",
        "    # Policy Extraction\n",
        "    policy = extract_policy(V, transition_probs, rewards, actions, gamma, states)\n",
        "\n",
        "    return V, policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vLq7AnvPQM1E",
      "metadata": {
        "id": "vLq7AnvPQM1E"
      },
      "source": [
        "### 2. Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I3bkYVJWgmCG",
      "metadata": {
        "id": "I3bkYVJWgmCG"
      },
      "source": [
        "\n",
        "#### 2(a). Policy Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yf1GplWRCCRg",
      "metadata": {
        "id": "yf1GplWRCCRg"
      },
      "source": [
        "*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 656,
      "id": "18FZJFjnJOye",
      "metadata": {
        "id": "18FZJFjnJOye"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(policy, transition_probs, rewards, gamma, theta, states):\n",
        "    \"\"\"\n",
        "    Evaluate the value function of a given policy using the Bellman expectation equation.\n",
        "\n",
        "    Parameters:\n",
        "    - policy: The policy to be evaluated.\n",
        "    - transition_probs: Transition model of the environment (p(s', r | s, a)).\n",
        "    - rewards: Reward function.\n",
        "    - gamma: Discount factor for future rewards.\n",
        "    - eps: Convergence threshold. Stops evaluation once value function changes are smaller than eps.\n",
        "    - states: List of all states.\n",
        "\n",
        "    Returns:\n",
        "    - V: Value function of the given policy (v_pi).\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization: Start with an arbitrary value function\n",
        "    V = {state: 0 for state in states} # Dictionary\n",
        "\n",
        "    while True:\n",
        "        new_V = V.copy()\n",
        "\n",
        "        # Update each state's value based on the Bellman expectation equation\n",
        "        for state in states: # Select a state from list of all states\n",
        "            # Given the deterministic policy, we directly get the action for the state\n",
        "            action = policy[state] # Following the given policy select the next action for the state\n",
        "\n",
        "            # Initialize the state's value to 0 and delta to 0 for this iteration\n",
        "            delta = 0\n",
        "            state_value=0\n",
        "\n",
        "            # Compute the expected value for the state using its policy action\n",
        "            for next_state in states:\n",
        "                transition_prob = transition_probs[state][action][next_state]\n",
        "                reward = rewards[state][action][next_state]\n",
        "                # Update the state value for the action given the policy\n",
        "                state_value += transition_prob * (reward + gamma * V[next_state])\n",
        "\n",
        "            # Assign the computed state value to the new value function\n",
        "            new_V[state] = state_value\n",
        "\n",
        "            # Convergence Check\n",
        "            delta = max(delta, abs(new_V[state] - V[state]))\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "        # Update the value function for the next iteration\n",
        "        V = new_V\n",
        "\n",
        "    return V\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oRYeZV0fgoVg",
      "metadata": {
        "id": "oRYeZV0fgoVg"
      },
      "source": [
        "#### 2(b). Policy Improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 657,
      "id": "rwhxA-PKJMyH",
      "metadata": {
        "id": "rwhxA-PKJMyH"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(V, transition_probs, rewards, actions, gamma, states):\n",
        "    \"\"\"\n",
        "    Improve the policy based on a given value function using the Bellman optimality equation.\n",
        "\n",
        "    Parameters:\n",
        "    - V: Current estimate of the value function.\n",
        "    - transition_probs: Transition model of the environment.\n",
        "    - rewards: Reward function.\n",
        "    - actions: Dictionary of available actions for each state.\n",
        "    - gamma: Discount factor for future rewards.\n",
        "    - states: List of all states.\n",
        "\n",
        "    Returns:\n",
        "    - new_policy: Improved policy.\n",
        "    \"\"\"\n",
        "\n",
        "    new_policy = {}\n",
        "\n",
        "    # Iterate through each state to update its policy based on action\n",
        "    for state in states:\n",
        "\n",
        "        # Initialize the best action and its corresponding value for this state\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "\n",
        "        # Evaluate each possible action to find the best one for this state\n",
        "        for action in actions[state]:\n",
        "            action_value = 0\n",
        "\n",
        "            # Calculate the expected value of this action over all next possible states\n",
        "            for next_state in states:\n",
        "                transition_prob = transition_probs[state][action][next_state]\n",
        "                reward = rewards[state][action][next_state]\n",
        "                action_value += transition_prob * (reward + gamma * V[next_state])\n",
        "\n",
        "            # Update best action if this action's value is higher\n",
        "            if action_value > best_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "\n",
        "        # Assign the best action to the policy for this state\n",
        "        new_policy[state] = best_action\n",
        "\n",
        "    return new_policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lvJR5phggqIl",
      "metadata": {
        "id": "lvJR5phggqIl"
      },
      "source": [
        "#### 2(c). Policy Iteration Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 658,
      "id": "_5gYuau0Ed6S",
      "metadata": {
        "id": "_5gYuau0Ed6S"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(states, actions, transition_probs, rewards, gamma=0.9, theta=1e-3, debug_print=False):\n",
        "    \"\"\"\n",
        "    Implements the policy iteration algorithm to find the optimal policy and value function.\n",
        "\n",
        "    Parameters:\n",
        "    - states: List of states.\n",
        "    - actions: Dictionary of available actions for each state.\n",
        "    - transition_probs: Transition model of the environment.\n",
        "    - rewards: Reward dictionary.\n",
        "    - gamma: Discount factor for future rewards.\n",
        "    - theta: Convergence threshold for policy evaluation phase.\n",
        "\n",
        "    Returns:\n",
        "    - V: Optimal value function.\n",
        "    - policy: Optimal policy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization: Arbitrarily choose an initial policy\n",
        "    # Here, we initialize the policy for each state as the first available action.\n",
        "    policy = {state: actions[state][0] for state in states}\n",
        "    iteration_count = 0\n",
        "\n",
        "    while True:\n",
        "        iteration_count += 1\n",
        "\n",
        "        # Policy Evaluation: Compute the state-value function for the current policy\n",
        "        V = policy_evaluation(policy, transition_probs, rewards, gamma, theta, states)\n",
        "\n",
        "        if debug_print:\n",
        "            print(f\"Policy Iteration {iteration_count}: Value Function after Policy Evaluation: {V}\")\n",
        "\n",
        "        # Policy Improvement: Update the policy to be greedy with respect to the current value function\n",
        "        new_policy = policy_improvement(V, transition_probs, rewards, actions, gamma, states)\n",
        "\n",
        "        if debug_print:\n",
        "            print(f\"Policy Iteration {iteration_count}: Policy after Improvement: {new_policy} \\n\")\n",
        "\n",
        "        # Convergence Check: If the policy remains unchanged after the improvement step, it's optimal\n",
        "        if new_policy == policy:\n",
        "            break\n",
        "\n",
        "        # Update the policy for the next iteration\n",
        "        policy = new_policy\n",
        "\n",
        "    return V, policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aEsILgcIgOYB",
      "metadata": {
        "id": "aEsILgcIgOYB"
      },
      "source": [
        "## 3. MDP Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ekk_aJdguzk",
      "metadata": {
        "id": "3ekk_aJdguzk"
      },
      "source": [
        "This time we generalise the MDP structure with solving functions, so that we can use any given scenerio. There are possibilities that depending on the scenerio, we modify the functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 659,
      "id": "B7klopSFFB0T",
      "metadata": {
        "id": "B7klopSFFB0T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, states, actions, transition_matrix, reward_matrix, discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Initialize the MDP with given states, actions, transition probabilities, rewards, and discount factor.\n",
        "\n",
        "        Parameters:\n",
        "        - states: List of states in the MDP\n",
        "        - actions: List of actions available in the MDP\n",
        "        - transition_matrix: Matrix where each row represents the current state, each column represents an action,\n",
        "                             and the inner lists represent the next state probabilities.\n",
        "        - reward_matrix: Matrix where each row represents the current state and each column represents an action.\n",
        "        - discount_factor: Discount factor for future rewards (gamma in Sutton & Barto)\n",
        "        \"\"\"\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        self.transition_matrix = transition_matrix\n",
        "        self.reward_matrix = reward_matrix\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "    # Converting the Transition Prob, rewards and actions to Dictionary.\n",
        "    def convert_to_dictionary(self):\n",
        "        \"\"\"\n",
        "        Convert transition matrix and reward matrix to a dictionary format which is more intuitive for certain operations.\n",
        "\n",
        "        Returns:\n",
        "        - transition_probs: Dictionary of transition probabilities\n",
        "        - rewards: Dictionary of rewards for state-action pairs\n",
        "        - actions: Dictionary of available actions for each state\n",
        "        \"\"\"\n",
        "        # Convert actions list to dictionary format\n",
        "        actions = {state: [act for act in self.actions] for state in self.states}\n",
        "\n",
        "        # Initialize the transition_probs and rewards dictionaries\n",
        "        transition_probs = {s: {} for s in self.states}\n",
        "        rewards = {s: {} for s in self.states}\n",
        "\n",
        "        for i, s in enumerate(self.states):\n",
        "            for j, a in enumerate(self.actions):\n",
        "                transition_probs[s][a] = {}\n",
        "                rewards[s][a] = {}\n",
        "                for k, s_prime in enumerate(self.states):\n",
        "                    # Set the transition probability for s' from the matrix\n",
        "                    transition_probs[s][a][s_prime] = self.transition_matrix[i][j][k]\n",
        "\n",
        "                    # Set the reward for action a in state s from the matrix\n",
        "                    rewards[s][a][s_prime] = self.reward_matrix[i][j][k]\n",
        "\n",
        "        return transition_probs, rewards, actions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "397d963b",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Define Specifics\n",
        "* States\n",
        "* Actions\n",
        "* Transition Probabilities (Matrix)\n",
        "* Rewards (Matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 668,
      "id": "Ls75lbb0AP_c",
      "metadata": {
        "id": "Ls75lbb0AP_c"
      },
      "outputs": [],
      "source": [
        "# Define states\n",
        "\n",
        "states2 = [\"Initial Increase\", \"Sustained Increase\", \"Initial Decrease\", \"Sustained Decrease\", \"Initial Stability\",'Sustained stability']\n",
        "# Define the actions\n",
        "actions2 = [\"Buy\",\"Sell\",\"Hold\"]\n",
        "\n",
        "\n",
        "# Define transition matrices for each state,actio,next state\n",
        "transition_matrix2 = [\n",
        "    # From Initial Increase to [\"Initial Increase\", \"Sustained Increase\", \"Initial Decrease\", \"Sustained Decrease\", \"Initial Stability\",'Sustained stability']\n",
        "    [\n",
        "        [0, 0.6, 0.2, 0, 0.2,0],  # Buy\n",
        "        [0, 0.6, 0.2, 0, 0.2,0],  # Sell\n",
        "        [0, 0.6, 0.2, 0, 0.2,0],  # Hold\n",
        "    ],\n",
        "    # From Sustained Increase\n",
        "    [\n",
        "        [0, 0.2, 0.4, 0, 0.4,0],  # Buy\n",
        "        [0, 0.2, 0.4, 0, 0.4,0],  # Sell\n",
        "        [0, 0.2, 0.4, 0, 0.4,0],  # Hold\n",
        "    ],\n",
        "    # From Initial Decrease\n",
        "    [\n",
        "        [0.2, 0, 0, 0.6, 0.2,0],  # Buy\n",
        "        [0.2, 0, 0, 0.6, 0.2,0],  # Sell\n",
        "        [0.2, 0, 0, 0.6, 0.2,0],  # Hold\n",
        "    ],\n",
        "    # From Sustained Decrease\n",
        "    [\n",
        "        [0.4, 0, 0, 0.2, 0.4,0],  # Buy\n",
        "        [0.4, 0, 0, 0.2, 0.4,0],  # Sell\n",
        "        [0.4, 0, 0, 0.2, 0.4,0],  # Hold\n",
        "    ],\n",
        "    # From Stability\n",
        "    [\n",
        "        [0.4, 0, 0.4, 0, 0,0.2],  # Buy\n",
        "        [0.4, 0, 0.4, 0, 0,0.2],  # Sell\n",
        "        [0.4, 0, 0.4, 0, 0,0.2],  # ,\n",
        "            # From Sustained Stability\n",
        "    ],[\n",
        "        [0.1, 0, 0.1, 0, 0, 0.8],  # Buy\n",
        "        [0.1, 0, 0.1, 0, 0, 0.8],  # Sell\n",
        "        [0.1, 0, 0.1, 0, 0, 0.8]        # Hold\n",
        "    ]\n",
        "]\n",
        "\n",
        "\n",
        "reward_matrix2 = [\n",
        "    # From Initial Increase\n",
        "    [\n",
        "        [-1000, +10, -10, -1000, -1000, -1000],  # Buy\n",
        "        [-1000, -10, +10, -1000, -1000, -1000],  # Sell\n",
        "        [-1000, -1000, -1000, -1000, -1000, -1000],        # Hold\n",
        "    ],\n",
        "    # From Sustained Increase\n",
        "    [\n",
        "        [-1000, +10, -10, -1000, -1000, -1000],  # Buy\n",
        "        [-1000, -10, +10, -1000, -1000, -1000],  # Sell\n",
        "        [-1000, -1000, -1000, -1000, -1000, -1000],        # Hold\n",
        "    ],\n",
        "    # From Initial Decrease\n",
        "    [\n",
        "        [+10, -1000, -1000, -10, -1000, -1000],  # Buy\n",
        "        [-10, -1000, -1000, +10, -1000, -1000],  # Sell\n",
        "        [-1000, -1000, -1000, -1000, -1000, -1000],        # Hold\n",
        "    ],\n",
        "    # From Sustained Decrease\n",
        "    [\n",
        "        [+10, -1000, -1000, -10, -1000, -1000],  # Buy\n",
        "        [-10, -1000, -1000, +10, -1000, -1000],  # Sell\n",
        "        [-1000, -1000, -1000, -1000, -1000, -1000],        # Hold\n",
        "    ],\n",
        "    # From Initial Stability\n",
        "    [\n",
        "        [+10, -1000, -1000, -10, -1000, -1000],  # Buy\n",
        "        [-10, -1000, -1000, +10, -1000, -1000],  # Sell\n",
        "        [-1000, -1000, -1000, -1000, -1000, -1000],        # Hold\n",
        "    ],\n",
        "    # From Sustained Stability\n",
        "    [\n",
        "        [+10, -1000, -1000, -10, -1000, -1000],  # Buy\n",
        "        [-10, -1000, -1000, +10, -1000, -1000],  # Sell\n",
        "        [-1000, -1000, -1000, -1000, -1000, -1000],        # Hold\n",
        "    ]\n",
        "]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37742920",
      "metadata": {},
      "source": [
        "### Create an instance of the MDP with the defined specifics\n",
        "* States\n",
        "* Actions\n",
        "* Transition Probabilities (Matrix)\n",
        "* Rewards (Matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 669,
      "id": "6x-pctllw14h",
      "metadata": {
        "id": "6x-pctllw14h"
      },
      "outputs": [],
      "source": [
        "# Create an MDP instance\n",
        "fin_MDP = MDP(states2, actions2, transition_matrix2, reward_matrix2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 670,
      "id": "hzj7-RGuw3Q4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzj7-RGuw3Q4",
        "outputId": "8bb06ee5-fffc-43ee-8c4a-4e927ac12a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Initial Increase': {'Buy': {'Initial Increase': 0, 'Sustained Increase': 0.6, 'Initial Decrease': 0.2, 'Sustained Decrease': 0, 'Initial Stability': 0.2, 'Sustained stability': 0}, 'Sell': {'Initial Increase': 0, 'Sustained Increase': 0.6, 'Initial Decrease': 0.2, 'Sustained Decrease': 0, 'Initial Stability': 0.2, 'Sustained stability': 0}, 'Hold': {'Initial Increase': 0, 'Sustained Increase': 0.6, 'Initial Decrease': 0.2, 'Sustained Decrease': 0, 'Initial Stability': 0.2, 'Sustained stability': 0}}, 'Sustained Increase': {'Buy': {'Initial Increase': 0, 'Sustained Increase': 0.2, 'Initial Decrease': 0.4, 'Sustained Decrease': 0, 'Initial Stability': 0.4, 'Sustained stability': 0}, 'Sell': {'Initial Increase': 0, 'Sustained Increase': 0.2, 'Initial Decrease': 0.4, 'Sustained Decrease': 0, 'Initial Stability': 0.4, 'Sustained stability': 0}, 'Hold': {'Initial Increase': 0, 'Sustained Increase': 0.2, 'Initial Decrease': 0.4, 'Sustained Decrease': 0, 'Initial Stability': 0.4, 'Sustained stability': 0}}, 'Initial Decrease': {'Buy': {'Initial Increase': 0.2, 'Sustained Increase': 0, 'Initial Decrease': 0, 'Sustained Decrease': 0.6, 'Initial Stability': 0.2, 'Sustained stability': 0}, 'Sell': {'Initial Increase': 0.2, 'Sustained Increase': 0, 'Initial Decrease': 0, 'Sustained Decrease': 0.6, 'Initial Stability': 0.2, 'Sustained stability': 0}, 'Hold': {'Initial Increase': 0.2, 'Sustained Increase': 0, 'Initial Decrease': 0, 'Sustained Decrease': 0.6, 'Initial Stability': 0.2, 'Sustained stability': 0}}, 'Sustained Decrease': {'Buy': {'Initial Increase': 0.4, 'Sustained Increase': 0, 'Initial Decrease': 0, 'Sustained Decrease': 0.2, 'Initial Stability': 0.4, 'Sustained stability': 0}, 'Sell': {'Initial Increase': 0.4, 'Sustained Increase': 0, 'Initial Decrease': 0, 'Sustained Decrease': 0.2, 'Initial Stability': 0.4, 'Sustained stability': 0}, 'Hold': {'Initial Increase': 0.4, 'Sustained Increase': 0, 'Initial Decrease': 0, 'Sustained Decrease': 0.2, 'Initial Stability': 0.4, 'Sustained stability': 0}}, 'Initial Stability': {'Buy': {'Initial Increase': 0.4, 'Sustained Increase': 0, 'Initial Decrease': 0.4, 'Sustained Decrease': 0, 'Initial Stability': 0, 'Sustained stability': 0.2}, 'Sell': {'Initial Increase': 0.4, 'Sustained Increase': 0, 'Initial Decrease': 0.4, 'Sustained Decrease': 0, 'Initial Stability': 0, 'Sustained stability': 0.2}, 'Hold': {'Initial Increase': 0.4, 'Sustained Increase': 0, 'Initial Decrease': 0.4, 'Sustained Decrease': 0, 'Initial Stability': 0, 'Sustained stability': 0.2}}, 'Sustained stability': {'Buy': {'Initial Increase': 0.1, 'Sustained Increase': 0, 'Initial Decrease': 0.1, 'Sustained Decrease': 0, 'Initial Stability': 0, 'Sustained stability': 0.8}, 'Sell': {'Initial Increase': 0.1, 'Sustained Increase': 0, 'Initial Decrease': 0.1, 'Sustained Decrease': 0, 'Initial Stability': 0, 'Sustained stability': 0.8}, 'Hold': {'Initial Increase': 0.1, 'Sustained Increase': 0, 'Initial Decrease': 0.1, 'Sustained Decrease': 0, 'Initial Stability': 0, 'Sustained stability': 0.8}}}\n"
          ]
        }
      ],
      "source": [
        "# Convert to Dictionary before proceeding with Policy Iteration\n",
        "transition_probs2, rewards2, actions2 = fin_MDP.convert_to_dictionary()\n",
        "print(transition_probs2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80adc789",
      "metadata": {},
      "source": [
        "### Solve MDP using Value Iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 671,
      "id": "tVfFYGkOw3UK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVfFYGkOw3UK",
        "outputId": "1e4330e3-1c4b-448a-c57d-27a8383c7c22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value Iteration 1: Max Change: 899.0, Value Function: {'Initial Increase': -196.0, 'Sustained Increase': -398.0, 'Initial Decrease': -196.0, 'Sustained Decrease': -398.0, 'Initial Stability': -596.0, 'Sustained stability': -899.0}\n",
            "Value Iteration 2: Max Change: 682.56, Value Function: {'Initial Increase': -553.48, 'Sustained Increase': -754.76, 'Initial Decrease': -553.48, 'Sustained Decrease': -754.76, 'Initial Stability': -898.94, 'Sustained stability': -1581.56}\n",
            "Value Iteration 3: Max Change: 555.7896000000001, Value Function: {'Initial Increase': -865.0060000000001, 'Sustained Increase': -1056.728, 'Initial Decrease': -865.0060000000001, 'Sustained Decrease': -1056.728, 'Initial Stability': -1279.1864, 'Sustained stability': -2137.3496}\n",
            "Value Iteration 4: Max Change: 456.24319200000036, Value Function: {'Initial Increase': -1152.5877520000001, 'Sustained Increase': -1360.1203040000003, 'Initial Decrease': -1152.5877520000001, 'Sustained Decrease': -1360.1203040000003, 'Initial Stability': -1603.5272480000003, 'Sustained stability': -2593.5927920000004}\n",
            "Value Iteration 5: Max Change: 380.25981360000014, Value Function: {'Initial Increase': -1426.5656641600003, 'Sustained Increase': -1635.0230547200003, 'Initial Decrease': -1426.5656641600003, 'Sustained Decrease': -1635.0230547200003, 'Initial Stability': -1892.7098840000003, 'Sustained stability': -2973.8526056000005}\n",
            "Value Iteration 6: Max Change: 323.1030899807997, Value Function: {'Initial Increase': -1676.3820482176002, 'Sustained Increase': -1887.2433471872005, 'Initial Decrease': -1676.3820482176002, 'Sustained Decrease': -1887.2433471872005, 'Initial Stability': -2158.4207472032003, 'Sustained stability': -3296.9556955808002}\n",
            "Value Iteration 7: Max Change: 277.6011739165442, Value Function: {'Initial Increase': -1905.3759106568323, 'Sustained Increase': -2118.2328088451845, 'Initial Decrease': -1905.3759106568323, 'Sustained Decrease': -2118.2328088451845, 'Initial Stability': -2396.4470999212162, 'Sustained stability': -3574.5568694973445}\n",
            "Value Iteration 8: Max Change: 241.09174045897407, Value Function: {'Initial Increase': -2114.1738586804486, 'Sustained Increase': -2327.938189400231, 'Initial Decrease': -2114.1738586804486, 'Sustained Decrease': -2327.938189400231, 'Initial Stability': -2611.2908921824414, 'Sustained stability': -3815.6486099563185}\n",
            "Value Iteration 9: Max Change: 211.16968377471176, Value Function: {'Initial Increase': -2303.670277431445, 'Sustained Increase': -2518.196184402682, 'Initial Decrease': -2303.670277431445, 'Sustained Decrease': -2518.196184402682, 'Initial Stability': -2805.02192804206, 'Sustained stability': -4026.8182937310303}\n",
            "Value Iteration 10: Max Change: 186.1515276929722, Value Function: {'Initial Increase': -2475.3905365626792, 'Sustained Increase': -2690.404507162945, 'Initial Decrease': -2475.3905365626792, 'Sustained Decrease': -2690.404507162945, 'Initial Stability': -2979.4698926222263, 'Sustained stability': -4212.9698214240025}\n",
            "Value Iteration 11: Max Change: 164.9387465825621, Value Function: {'Initial Increase': -2630.6933111212734, 'Sustained Increase': -2846.0225657958963, 'Initial Decrease': -2630.6933111212734, 'Sustained Decrease': -2846.0225657958963, 'Initial Stability': -3136.61575418145, 'Sustained stability': -4377.908568006565}\n",
            "Value Iteration 12: Max Change: 146.71039695999207, Value Function: {'Initial Increase': -2770.967817284274, 'Sustained Increase': -2986.515325352242, 'Initial Decrease': -2770.967817284274, 'Sustained Decrease': -2986.515325352242, 'Initial Stability': -3278.122726248499, 'Sustained stability': -4524.618964966557}\n",
            "Value Iteration 13: Max Change: 130.88089692053381, Value Function: {'Initial Increase': -2897.55457352611, 'Sustained Increase': -3113.2453542352023, 'Initial Decrease': -2897.55457352611, 'Sustained Decrease': -3113.2453542352023, 'Initial Stability': -3405.528242138658, 'Sustained stability': -4655.49986188709}\n",
            "Value Iteration 14: Max Change: 117.01986190631487, Value Function: {'Initial Increase': -3011.707398106668, 'Sustained Increase': -3227.493977401653, 'Initial Decrease': -3011.707398106668, 'Sustained Decrease': -3227.493977401653, 'Initial Stability': -3520.229268078476, 'Sustained stability': -4772.519723793405}\n",
            "Value Iteration 15: Max Change: 104.80180899704737, Value Function: {'Initial Increase': -3114.595347710219, 'Sustained Increase': -3330.4461157589494, 'Initial Decrease': -3114.595347710219, 'Sustained Decrease': -3330.4461157589494, 'Initial Stability': -3623.482876919614, 'Sustained stability': -4877.321532790453}\n",
            "Value Iteration 16: Max Change: 93.97713340651353, Value Function: {'Initial Increase': -3207.2949829432027, 'Sustained Increase': -3423.188461703351, 'Initial Decrease': -3207.2949829432027, 'Sustained Decrease': -3423.188461703351, 'Initial Stability': -3716.426526253639, 'Sustained stability': -4971.298666196966}\n",
            "Value Iteration 17: Max Change: 84.34947039462622, Value Function: {'Initial Increase': -3290.7916409752415, 'Sustained Increase': -3506.713666417467, 'Initial Decrease': -3290.7916409752415, 'Sustained Decrease': -3506.713666417467, 'Initial Stability': -3800.08614763456, 'Sustained stability': -5055.648136591592}\n",
            "Value Iteration 18: Max Change: 75.76101712989839, Value Function: {'Initial Increase': -3365.9833818151965, 'Sustained Increase': -3581.924463854673, 'Initial Decrease': -3365.9833818151965, 'Sustained Decrease': -3581.924463854673, 'Initial Stability': -3875.386646088661, 'Sustained stability': -5131.409153721491}\n",
            "Value Iteration 19: Max Change: 68.08244568471855, Value Function: {'Initial Increase': -3433.685815504218, 'Sustained Increase': -3649.6396135392306, 'Initial Decrease': -3433.685815504218, 'Sustained Decrease': -3649.6396135392306, 'Initial Stability': -3943.16168257681, 'Sustained stability': -5199.491599406209}\n",
            "Value Iteration 20: Max Change: 61.205798957021216, Value Function: {'Initial Increase': -3494.6379409657698, 'Sustained Increase': -3710.6002297462323, 'Initial Decrease': -3494.6379409657698, 'Sustained Decrease': -3710.6002297462323, 'Initial Stability': -4004.1622750561555, 'Sustained stability': -5260.697398363231}\n",
            "Value Iteration 21: Max Change: 55.03955783213496, Value Function: {'Initial Increase': -3549.508162946912, 'Sustained Increase': -3765.476119122215, 'Initial Decrease': -3549.508162946912, 'Sustained Decrease': -3765.476119122215, 'Initial Stability': -4059.0648492007363, 'Sustained stability': -5315.736956195366}\n",
            "Value Iteration 22: Max Change: 49.50512159574282, Value Function: {'Initial Increase': -3598.900246512573, 'Sustained Increase': -3814.871985815152, 'Initial Decrease': -3598.900246512573, 'Sustained Decrease': -3814.871985815152, 'Initial Stability': -4108.478529436942, 'Sustained stability': -5365.242077791108}\n",
            "Value Iteration 23: Max Change: 44.534262590753315, Value Function: {'Initial Increase': -3643.359052011095, 'Sustained Increase': -3859.333316788553, 'Initial Decrease': -3643.359052011095, 'Sustained Decrease': -3859.333316788553, 'Initial Stability': -4152.951751491452, 'Sustained stability': -5409.776340381862}\n",
            "Value Iteration 24: Max Change: 40.06725405507677, Value Function: {'Initial Increase': -3683.3759356962773, 'Sustained Increase': -3899.3518862828573, 'Initial Decrease': -3683.3759356962773, 'Sustained Decrease': -3899.3518862828573, 'Initial Stability': -4192.978258716724, 'Sustained stability': -5449.8435944369385}\n",
            "Value Iteration 25: Max Change: 36.05146198298735, Value Function: {'Initial Increase': -3719.393773587083, 'Sustained Increase': -3935.370849519595, 'Initial Decrease': -3719.393773587083, 'Sustained Decrease': -3935.370849519595, 'Initial Stability': -4229.002520699969, 'Sustained stability': -5485.895056419926}\n",
            "Value Iteration 26: Max Change: 32.44026344809663, Value Function: {'Initial Increase': -3751.811591712251, 'Sustained Increase': -3967.789418856866, 'Initial Decrease': -3751.811591712251, 'Sustained Decrease': -3967.789418856866, 'Initial Stability': -4261.424627138287, 'Sustained stability': -5518.335319868022}\n",
            "Value Iteration 27: Max Change: 29.192196945158685, Value Function: {'Initial Increase': -3780.9888055758047, 'Sustained Increase': -3996.96713418043, 'Initial Decrease': -3780.9888055758047, 'Sustained Decrease': -3996.96713418043, 'Initial Stability': -4290.604703609065, 'Sustained stability': -5547.527516813181}\n",
            "Value Iteration 28: Max Change: 26.270280295954763, Value Function: {'Initial Increase': -3807.2490841107087, 'Sustained Increase': -4023.22774745903, 'Initial Decrease': -3807.2490841107087, 'Sustained Decrease': -4023.22774745903, 'Initial Stability': -4316.866893040952, 'Sustained stability': -5573.797797109136}\n",
            "Value Iteration 29: Max Change: 23.641451949369184, Value Function: {'Initial Increase': -3830.883859515175, 'Sustained Increase': -4046.8627463172234, 'Initial Decrease': -3830.883859515175, 'Sustained Decrease': -4046.8627463172234, 'Initial Stability': -4340.5029440393555, 'Sustained stability': -5597.439249058505}\n",
            "Value Iteration 30: Max Change: 21.276104976350325, Value Function: {'Initial Increase': -3852.1555076511163, 'Sustained Increase': -4068.134543616732, 'Initial Decrease': -3852.1555076511163, 'Sustained Decrease': -4068.134543616732, 'Initial Stability': -4361.775443681458, 'Sustained stability': -5618.715354034855}\n",
            "Value Iteration 31: Max Change: 19.147692247441228, Value Function: {'Initial Increase': -3871.300224792899, 'Sustained Increase': -4087.2793603307387, 'Initial Decrease': -3871.300224792899, 'Sustained Decrease': -4087.2793603307387, 'Initial Stability': -4380.9207292350775, 'Sustained stability': -5637.863046282297}\n",
            "Value Iteration 32: Max Change: 17.232387503680002, Value Function: {'Initial Increase': -3888.5306263036346, 'Sustained Increase': -4104.509828309605, 'Initial Decrease': -3888.5306263036346, 'Sustained Decrease': -4104.509828309605, 'Initial Stability': -4398.151510181701, 'Sustained stability': -5655.095433785977}\n",
            "Value Iteration 33: Max Change: 15.508791274581199, Value Function: {'Initial Increase': -3904.038091854547, 'Sustained Increase': -4120.01733823045, 'Initial Decrease': -3904.038091854547, 'Sustained Decrease': -4120.01733823045, 'Initial Stability': -4413.659229020093, 'Sustained stability': -5670.604225060558}\n",
            "Value Iteration 34: Max Change: 13.957673516862087, Value Function: {'Initial Increase': -3917.9948804018786, 'Sustained Increase': -4133.974156396353, 'Initial Decrease': -3917.9948804018786, 'Sustained Decrease': -4133.974156396353, 'Initial Stability': -4427.616186646175, 'Sustained stability': -5684.56189857742}\n",
            "Value Iteration 35: Max Change: 12.5617468706605, Value Function: {'Initial Increase': -3930.5560365226797, 'Sustained Increase': -4146.535332288643, 'Initial Decrease': -3930.5560365226797, 'Sustained Decrease': -4146.535332288643, 'Initial Stability': -4440.177455633288, 'Sustained stability': -5697.12364544808}\n",
            "Value Iteration 36: Max Change: 11.30546584861986, Value Function: {'Initial Increase': -3941.8611080239416, 'Sustained Increase': -4157.840416988105, 'Initial Decrease': -3941.8611080239416, 'Sustained Decrease': -4157.840416988105, 'Initial Stability': -4451.4826024769845, 'Sustained stability': -5708.4291112967}\n",
            "Value Iteration 37: Max Change: 10.174848281233608, Value Function: {'Initial Increase': -3952.035693063743, 'Sustained Increase': -4168.015010838192, 'Initial Decrease': -3952.035693063743, 'Sustained Decrease': -4168.015010838192, 'Initial Stability': -4461.657237810644, 'Sustained stability': -5718.603959577934}\n",
            "Value Iteration 38: Max Change: 9.157316069652552, Value Function: {'Initial Increase': -3961.1928334100135, 'Sustained Increase': -4177.172157065655, 'Initial Decrease': -3961.1928334100135, 'Sustained Decrease': -4177.172157065655, 'Initial Stability': -4470.814411729923, 'Sustained stability': -5727.761275647586}\n",
            "Value Iteration 39: Max Change: 8.241552832478192, Value Function: {'Initial Increase': -3969.4342689406426, 'Sustained Increase': -4185.413596522196, 'Initial Decrease': -3969.4342689406426, 'Sustained Decrease': -4185.413596522196, 'Initial Stability': -4479.055869671775, 'Sustained stability': -5736.002828480065}\n",
            "Value Iteration 40: Max Change: 7.41737643489796, Value Function: {'Initial Increase': -3976.8515670722213, 'Sustained Increase': -4192.830897274466, 'Initial Decrease': -3976.8515670722213, 'Sustained Decrease': -4192.830897274466, 'Initial Stability': -4486.473182763674, 'Sustained stability': -5743.420204914963}\n",
            "Value Iteration 41: Max Change: 6.675624696810701, Value Function: {'Initial Increase': -3983.5271394986726, 'Sustained Increase': -4199.506471450326, 'Initial Decrease': -3983.5271394986726, 'Sustained Decrease': -4199.506471450326, 'Initial Stability': -4493.148765176693, 'Sustained stability': -5750.095829611773}\n",
            "Value Iteration 42: Max Change: 6.008052818464421, Value Function: {'Initial Increase': -3989.5351574247416, 'Sustained Increase': -4205.51449054419, 'Initial Decrease': -3989.5351574247416, 'Sustained Decrease': -4205.51449054419, 'Initial Stability': -4499.156789769164, 'Sustained stability': -5756.103882430238}\n",
            "Value Iteration 43: Max Change: 5.407241255987174, Value Function: {'Initial Increase': -3994.942375388766, 'Sustained Increase': -4210.921709287761, 'Initial Decrease': -3994.942375388766, 'Sustained Decrease': -4210.921709287761, 'Initial Stability': -4504.564012183257, 'Sustained stability': -5761.511123686225}\n",
            "Value Iteration 44: Max Change: 4.866512937835068, Value Function: {'Initial Increase': -3999.808872778355, 'Sustained Increase': -4215.788207197726, 'Initial Decrease': -3999.808872778355, 'Sustained Decrease': -4215.788207197726, 'Initial Stability': -4509.430512543432, 'Sustained stability': -5766.37763662406}\n",
            "Value Iteration 45: Max Change: 4.37985884536738, Value Function: {'Initial Increase': -4004.1887212446936, 'Sustained Increase': -4220.168056011435, 'Initial Decrease': -4004.1887212446936, 'Sustained Decrease': -4220.168056011435, 'Initial Stability': -4513.810362992747, 'Sustained stability': -5770.757495469427}\n",
            "Value Iteration 46: Max Change: 3.9418710926065614, Value Function: {'Initial Increase': -4008.130585408914, 'Sustained Increase': -4224.109920407537, 'Initial Decrease': -4008.130585408914, 'Sustained Decrease': -4224.109920407537, 'Initial Stability': -4517.752228480676, 'Sustained stability': -5774.699366562034}\n",
            "Value Iteration 47: Max Change: 3.5476827362354015, Value Function: {'Initial Increase': -4011.678263520196, 'Sustained Increase': -4227.65759867361, 'Initial Decrease': -4011.678263520196, 'Sustained Decrease': -4227.65759867361, 'Initial Stability': -4521.299907475584, 'Sustained stability': -5778.247049298269}\n",
            "Value Iteration 48: Max Change: 3.192913630120529, Value Function: {'Initial Increase': -4014.87117406299, 'Sustained Increase': -4230.850509319731, 'Initial Decrease': -4014.87117406299, 'Sustained Decrease': -4230.850509319731, 'Initial Stability': -4524.492818608231, 'Sustained stability': -5781.43996292839}\n",
            "Value Iteration 49: Max Change: 2.8736217113892053, Value Function: {'Initial Increase': -4017.7447937134743, 'Sustained Increase': -4233.724129039191, 'Initial Decrease': -4017.7447937134743, 'Sustained Decrease': -4233.724129039191, 'Initial Stability': -4527.366438652463, 'Sustained stability': -5784.313584639779}\n",
            "Value Iteration 50: Max Change: 2.5862591692875867, Value Function: {'Initial Increase': -4020.331051507032, 'Sustained Increase': -4236.310386878793, 'Initial Decrease': -4020.331051507032, 'Sustained Decrease': -4236.310386878793, 'Initial Stability': -4529.952696708862, 'Sustained stability': -5786.899843809067}\n",
            "Value Iteration 51: Max Change: 2.3276330047274314, Value Function: {'Initial Increase': -4022.658683593409, 'Sustained Increase': -4238.6380189959045, 'Initial Decrease': -4022.658683593409, 'Sustained Decrease': -4238.6380189959045, 'Initial Stability': -4532.280328970695, 'Sustained stability': -5789.227476813794}\n",
            "Value Iteration 52: Max Change: 2.0948695389515706, Value Function: {'Initial Increase': -4024.753552519327, 'Sustained Increase': -4240.73288794234, 'Initial Decrease': -4024.753552519327, 'Sustained Decrease': -4240.73288794234, 'Initial Stability': -4534.375198013739, 'Sustained stability': -5791.322346352746}\n",
            "Value Iteration 53: Max Change: 1.8853824747102408, Value Function: {'Initial Increase': -4026.6389345848156, 'Sustained Increase': -4242.618270021525, 'Initial Decrease': -4026.6389345848156, 'Sustained Decrease': -4242.618270021525, 'Initial Stability': -4536.26058015741, 'Sustained stability': -5793.207728827456}\n",
            "Value Iteration 54: Max Change: 1.6968441535791499, Value Function: {'Initial Increase': -4028.335778465224, 'Sustained Increase': -4244.315113911076, 'Initial Decrease': -4028.335778465224, 'Sustained Decrease': -4244.315113911076, 'Initial Stability': -4537.957424090009, 'Sustained stability': -5794.904572981035}\n",
            "Value Iteration 55: Max Change: 1.5271596890515866, Value Function: {'Initial Increase': -4029.862937971923, 'Sustained Increase': -4245.8422734238775, 'Initial Decrease': -4029.862937971923, 'Sustained Decrease': -4245.8422734238775, 'Initial Stability': -4539.484583631548, 'Sustained stability': -5796.431732670087}\n",
            "Value Iteration 56: Max Change: 1.3744436873221275, Value Function: {'Initial Increase': -4031.2373815375186, 'Sustained Increase': -4247.216716993547, 'Initial Decrease': -4031.2373815375186, 'Sustained Decrease': -4247.216716993547, 'Initial Stability': -4540.8590272204, 'Sustained stability': -5797.806176357409}\n",
            "Value Iteration 57: Max Change: 1.2369992966796417, Value Function: {'Initial Increase': -4032.474380752941, 'Sustained Increase': -4248.45371621169, 'Initial Decrease': -4032.474380752941, 'Sustained Decrease': -4248.45371621169, 'Initial Stability': -4542.096026451348, 'Sustained stability': -5799.043175654088}\n",
            "Value Iteration 58: Max Change: 1.1132993523851837, Value Function: {'Initial Increase': -4033.587680051085, 'Sustained Increase': -4249.567015511649, 'Initial Decrease': -4033.587680051085, 'Sustained Decrease': -4249.567015511649, 'Initial Stability': -4543.209325759854, 'Sustained stability': -5800.156475006474}\n",
            "Value Iteration 59: Max Change: 1.0019694073835126, Value Function: {'Initial Increase': -4034.5896494222598, 'Sustained Increase': -4250.568984884036, 'Initial Decrease': -4034.5896494222598, 'Sustained Decrease': -4250.568984884036, 'Initial Stability': -4544.211295137948, 'Sustained stability': -5801.158444413857}\n",
            "Value Iteration 60: Max Change: 0.9017724601271766, Value Function: {'Initial Increase': -4035.4914218582167, 'Sustained Increase': -4251.470757320802, 'Initial Decrease': -4035.4914218582167, 'Sustained Decrease': -4251.470757320802, 'Initial Stability': -4545.113067578522, 'Sustained stability': -5802.060216873984}\n",
            "Value Iteration 61: Max Change: 0.8115952097650734, Value Function: {'Initial Increase': -4036.303017051846, 'Sustained Increase': -4252.282352514971, 'Initial Decrease': -4036.303017051846, 'Sustained Decrease': -4252.282352514971, 'Initial Stability': -4545.9246627752345, 'Sustained stability': -5802.871812083749}\n",
            "Value Iteration 62: Max Change: 0.7304356858830943, Value Function: {'Initial Increase': -4037.033452726959, 'Sustained Increase': -4253.012788190445, 'Initial Decrease': -4037.033452726959, 'Sustained Decrease': -4253.012788190445, 'Initial Stability': -4546.655098452405, 'Sustained stability': -5803.602247769632}\n",
            "Value Iteration 63: Max Change: 0.6573921153558331, Value Function: {'Initial Increase': -4037.690844835125, 'Sustained Increase': -4253.670180298851, 'Initial Decrease': -4037.690844835125, 'Sustained Decrease': -4253.670180298851, 'Initial Stability': -4547.312490561944, 'Sustained stability': -5804.259639884988}\n",
            "Value Iteration 64: Max Change: 0.5916529025262207, Value Function: {'Initial Increase': -4038.2824977328523, 'Sustained Increase': -4254.261833196739, 'Initial Decrease': -4038.2824977328523, 'Sustained Decrease': -4254.261833196739, 'Initial Stability': -4547.904143460588, 'Sustained stability': -5804.8512927875145}\n",
            "Value Iteration 65: Max Change: 0.5324876114100334, Value Function: {'Initial Increase': -4038.814985341058, 'Sustained Increase': -4254.794320805052, 'Initial Decrease': -4038.814985341058, 'Sustained Decrease': -4254.794320805052, 'Initial Stability': -4548.436631069407, 'Sustained stability': -5805.3837803989245}\n",
            "Value Iteration 66: Max Change: 0.47923884969259234, Value Function: {'Initial Increase': -4039.2942241886117, 'Sustained Increase': -4255.273559652677, 'Initial Decrease': -4039.2942241886117, 'Sustained Decrease': -4255.273559652677, 'Initial Stability': -4548.915869917369, 'Sustained stability': -5805.863019248617}\n",
            "Value Iteration 67: Max Change: 0.4313149643376164, Value Function: {'Initial Increase': -4039.7255391515223, 'Sustained Increase': -4255.704874615635, 'Initial Decrease': -4039.7255391515223, 'Sustained Decrease': -4255.704874615635, 'Initial Stability': -4549.347184880552, 'Sustained stability': -5806.294334212955}\n",
            "Value Iteration 68: Max Change: 0.3881834676467406, Value Function: {'Initial Increase': -4040.1137226182163, 'Sustained Increase': -4256.093058082361, 'Initial Decrease': -4040.1137226182163, 'Sustained Decrease': -4256.093058082361, 'Initial Stability': -4549.735368347428, 'Sustained stability': -5806.682517680601}\n",
            "Value Iteration 69: Max Change: 0.34936512071089965, Value Function: {'Initial Increase': -4040.4630877382906, 'Sustained Increase': -4256.442423202458, 'Initial Decrease': -4040.4630877382906, 'Sustained Decrease': -4256.442423202458, 'Initial Stability': -4550.084733467625, 'Sustained stability': -5807.031882801312}\n",
            "Value Iteration 70: Max Change: 0.3144286085253043, Value Function: {'Initial Increase': -4040.777516346392, 'Sustained Increase': -4256.756851810573, 'Initial Decrease': -4040.777516346392, 'Sustained Decrease': -4256.756851810573, 'Initial Stability': -4550.399162075806, 'Sustained stability': -5807.346311409838}\n",
            "Value Iteration 71: Max Change: 0.28298574759628536, Value Function: {'Initial Increase': -4041.0605020937046, 'Sustained Increase': -4257.039837557894, 'Initial Decrease': -4041.0605020937046, 'Sustained Decrease': -4257.039837557894, 'Initial Stability': -4550.682147823174, 'Sustained stability': -5807.629297157434}\n",
            "Value Iteration 72: Max Change: 0.2546871727854523, Value Function: {'Initial Increase': -4041.315189266301, 'Sustained Increase': -4257.294524730498, 'Initial Decrease': -4041.315189266301, 'Sustained Decrease': -4257.294524730498, 'Initial Stability': -4550.936834995806, 'Sustained stability': -5807.883984330219}\n",
            "Value Iteration 73: Max Change: 0.2292184554735286, Value Function: {'Initial Increase': -4041.544407721648, 'Sustained Increase': -4257.523743185848, 'Initial Decrease': -4041.544407721648, 'Sustained Decrease': -4257.523743185848, 'Initial Stability': -4551.166053451176, 'Sustained stability': -5808.113202785693}\n",
            "Value Iteration 74: Max Change: 0.20629660990380216, Value Function: {'Initial Increase': -4041.7507043314663, 'Sustained Increase': -4257.73003979567, 'Initial Decrease': -4041.7507043314663, 'Sustained Decrease': -4257.73003979567, 'Initial Stability': -4551.372350061012, 'Sustained stability': -5808.319499395597}\n",
            "Value Iteration 75: Max Change: 0.18566694889705104, Value Function: {'Initial Increase': -4041.936371280308, 'Sustained Increase': -4257.915706744513, 'Initial Decrease': -4041.936371280308, 'Sustained Decrease': -4257.915706744513, 'Initial Stability': -4551.558017009863, 'Sustained stability': -5808.505166344494}\n",
            "Value Iteration 76: Max Change: 0.1671002539969777, Value Function: {'Initial Increase': -4042.103471534268, 'Sustained Increase': -4258.082806998475, 'Initial Decrease': -4042.103471534268, 'Sustained Decrease': -4258.082806998475, 'Initial Stability': -4551.725117263831, 'Sustained stability': -5808.672266598491}\n",
            "Value Iteration 77: Max Change: 0.15039022859127726, Value Function: {'Initial Increase': -4042.253861762834, 'Sustained Increase': -4258.233197227041, 'Initial Decrease': -4042.253861762834, 'Sustained Decrease': -4258.233197227041, 'Initial Stability': -4551.875507492402, 'Sustained stability': -5808.822656827082}\n",
            "Value Iteration 78: Max Change: 0.13535120572851156, Value Function: {'Initial Increase': -4042.3892129685446, 'Sustained Increase': -4258.3685484327525, 'Initial Decrease': -4042.3892129685446, 'Sustained Decrease': -4258.3685484327525, 'Initial Stability': -4552.010858698115, 'Sustained stability': -5808.9580080328105}\n",
            "Value Iteration 79: Max Change: 0.12181608515220432, Value Function: {'Initial Increase': -4042.5110290536854, 'Sustained Increase': -4258.490364517894, 'Initial Decrease': -4042.5110290536854, 'Sustained Decrease': -4258.490364517894, 'Initial Stability': -4552.132674783259, 'Sustained stability': -5809.079824117963}\n",
            "Value Iteration 80: Max Change: 0.10963447663380066, Value Function: {'Initial Increase': -4042.6206635303124, 'Sustained Increase': -4258.599998994521, 'Initial Decrease': -4042.6206635303124, 'Sustained Decrease': -4258.599998994521, 'Initial Stability': -4552.242309259887, 'Sustained stability': -5809.1894585945965}\n",
            "Value Iteration 81: Max Change: 0.0986710289698749, Value Function: {'Initial Increase': -4042.7193345592777, 'Sustained Increase': -4258.698670023487, 'Initial Decrease': -4042.7193345592777, 'Sustained Decrease': -4258.698670023487, 'Initial Stability': -4552.340980288853, 'Sustained stability': -5809.288129623566}\n",
            "Value Iteration 82: Max Change: 0.08880392607170506, Value Function: {'Initial Increase': -4042.8081384853467, 'Sustained Increase': -4258.787473949555, 'Initial Decrease': -4042.8081384853467, 'Sustained Decrease': -4258.787473949555, 'Initial Stability': -4552.429784214923, 'Sustained stability': -5809.376933549638}\n",
            "Value Iteration 83: Max Change: 0.07992353346435266, Value Function: {'Initial Increase': -4042.888062018808, 'Sustained Increase': -4258.867397483017, 'Initial Decrease': -4042.888062018808, 'Sustained Decrease': -4258.867397483017, 'Initial Stability': -4552.509707748385, 'Sustained stability': -5809.4568570831025}\n",
            "Value Iteration 84: Max Change: 0.07193118011764454, Value Function: {'Initial Increase': -4042.959993198924, 'Sustained Increase': -4258.939328663133, 'Initial Decrease': -4042.959993198924, 'Sustained Decrease': -4258.939328663133, 'Initial Stability': -4552.581638928501, 'Sustained stability': -5809.52878826322}\n",
            "Value Iteration 85: Max Change: 0.06473806210578914, Value Function: {'Initial Increase': -4043.024731261028, 'Sustained Increase': -4259.004066725238, 'Initial Decrease': -4043.024731261028, 'Sustained Decrease': -4259.004066725238, 'Initial Stability': -4552.646376990606, 'Sustained stability': -5809.593526325326}\n",
            "Value Iteration 86: Max Change: 0.05826425589475548, Value Function: {'Initial Increase': -4043.0829955169224, 'Sustained Increase': -4259.062330981131, 'Initial Decrease': -4043.0829955169224, 'Sustained Decrease': -4259.062330981131, 'Initial Stability': -4552.7046412465, 'Sustained stability': -5809.651790581221}\n",
            "Value Iteration 87: Max Change: 0.05243783030527993, Value Function: {'Initial Increase': -4043.135433347227, 'Sustained Increase': -4259.114768811436, 'Initial Decrease': -4043.135433347227, 'Sustained Decrease': -4259.114768811436, 'Initial Stability': -4552.757079076804, 'Sustained stability': -5809.704228411525}\n",
            "Value Iteration 88: Max Change: 0.04719404727438814, Value Function: {'Initial Increase': -4043.182627394501, 'Sustained Increase': -4259.1619628587105, 'Initial Decrease': -4043.182627394501, 'Sustained Decrease': -4259.1619628587105, 'Initial Stability': -4552.8042731240785, 'Sustained stability': -5809.751422458799}\n",
            "Value Iteration 89: Max Change: 0.042474642546949326, Value Function: {'Initial Increase': -4043.2251020370477, 'Sustained Increase': -4259.204437501257, 'Initial Decrease': -4043.2251020370477, 'Sustained Decrease': -4259.204437501257, 'Initial Stability': -4552.846747766625, 'Sustained stability': -5809.793897101346}\n",
            "Value Iteration 90: Max Change: 0.03822717829234534, Value Function: {'Initial Increase': -4043.26332921534, 'Sustained Increase': -4259.242664679548, 'Initial Decrease': -4043.26332921534, 'Sustained Decrease': -4259.242664679548, 'Initial Stability': -4552.884974944917, 'Sustained stability': -5809.832124279638}\n",
            "Value Iteration 91: Max Change: 0.03440446046261059, Value Function: {'Initial Increase': -4043.297733675802, 'Sustained Increase': -4259.277069140011, 'Initial Decrease': -4043.297733675802, 'Sustained Decrease': -4259.277069140011, 'Initial Stability': -4552.91937940538, 'Sustained stability': -5809.866528740101}\n",
            "Value Iteration 92: Max Change: 0.030964014416895225, Value Function: {'Initial Increase': -4043.328697690219, 'Sustained Increase': -4259.308033154428, 'Initial Decrease': -4043.328697690219, 'Sustained Decrease': -4259.308033154428, 'Initial Stability': -4552.950343419796, 'Sustained stability': -5809.897492754518}\n",
            "Value Iteration 93: Max Change: 0.0278676129755695, Value Function: {'Initial Increase': -4043.356565303194, 'Sustained Increase': -4259.335900767403, 'Initial Decrease': -4043.356565303194, 'Sustained Decrease': -4259.335900767403, 'Initial Stability': -4552.9782110327715, 'Sustained stability': -5809.925360367493}\n",
            "Value Iteration 94: Max Change: 0.025080851677557803, Value Function: {'Initial Increase': -4043.3816461548713, 'Sustained Increase': -4259.36098161908, 'Initial Decrease': -4043.3816461548713, 'Sustained Decrease': -4259.36098161908, 'Initial Stability': -4553.003291884449, 'Sustained stability': -5809.950441219171}\n",
            "Value Iteration 95: Max Change: 0.022572766509256326, Value Function: {'Initial Increase': -4043.4042189213806, 'Sustained Increase': -4259.383554385589, 'Initial Decrease': -4043.4042189213806, 'Sustained Decrease': -4259.383554385589, 'Initial Stability': -4553.025864650957, 'Sustained stability': -5809.97301398568}\n",
            "Value Iteration 96: Max Change: 0.02031548985905829, Value Function: {'Initial Increase': -4043.4245344112396, 'Sustained Increase': -4259.403869875448, 'Initial Decrease': -4043.4245344112396, 'Sustained Decrease': -4259.403869875448, 'Initial Stability': -4553.046180140816, 'Sustained stability': -5809.993329475538}\n",
            "Value Iteration 97: Max Change: 0.018283940873516258, Value Function: {'Initial Increase': -4043.442818352112, 'Sustained Increase': -4259.422153816321, 'Initial Decrease': -4043.442818352112, 'Sustained Decrease': -4259.422153816321, 'Initial Stability': -4553.06446408169, 'Sustained stability': -5810.011613416411}\n",
            "Value Iteration 98: Max Change: 0.016455546785437036, Value Function: {'Initial Increase': -4043.4592738988977, 'Sustained Increase': -4259.4386093631065, 'Initial Decrease': -4043.4592738988977, 'Sustained Decrease': -4259.4386093631065, 'Initial Stability': -4553.080919628475, 'Sustained stability': -5810.028068963196}\n",
            "Value Iteration 99: Max Change: 0.014809992107529979, Value Function: {'Initial Increase': -4043.474083891005, 'Sustained Increase': -4259.453419355214, 'Initial Decrease': -4043.474083891005, 'Sustained Decrease': -4259.453419355214, 'Initial Stability': -4553.095729620582, 'Sustained stability': -5810.042878955303}\n",
            "Value Iteration 100: Max Change: 0.013328992897186254, Value Function: {'Initial Increase': -4043.487412883901, 'Sustained Increase': -4259.46674834811, 'Initial Decrease': -4043.487412883901, 'Sustained Decrease': -4259.46674834811, 'Initial Stability': -4553.109058613479, 'Sustained stability': -5810.056207948201}\n",
            "Value Iteration 101: Max Change: 0.011996093606285285, Value Function: {'Initial Increase': -4043.4994089775073, 'Sustained Increase': -4259.478744441716, 'Initial Decrease': -4043.4994089775073, 'Sustained Decrease': -4259.478744441716, 'Initial Stability': -4553.121054707084, 'Sustained stability': -5810.068204041807}\n",
            "Value Iteration 102: Max Change: 0.010796484246384352, Value Function: {'Initial Increase': -4043.510205461753, 'Sustained Increase': -4259.4895409259625, 'Initial Decrease': -4043.510205461753, 'Sustained Decrease': -4259.4895409259625, 'Initial Stability': -4553.1318511913305, 'Sustained stability': -5810.079000526053}\n",
            "Value Iteration 103: Max Change: 0.009716835822018766, Value Function: {'Initial Increase': -4043.519922297575, 'Sustained Increase': -4259.499257761784, 'Initial Decrease': -4043.519922297575, 'Sustained Decrease': -4259.499257761784, 'Initial Stability': -4553.141568027152, 'Sustained stability': -5810.088717361875}\n",
            "Value Iteration 104: Max Change: 0.008745152239498566, Value Function: {'Initial Increase': -4043.5286674498143, 'Sustained Increase': -4259.508002914023, 'Initial Decrease': -4043.5286674498143, 'Sustained Decrease': -4259.508002914023, 'Initial Stability': -4553.150313179392, 'Sustained stability': -5810.097462514113}\n",
            "Value Iteration 105: Max Change: 0.007870637015912507, Value Function: {'Initial Increase': -4043.5365380868298, 'Sustained Increase': -4259.515873551039, 'Initial Decrease': -4043.5365380868298, 'Sustained Decrease': -4259.515873551039, 'Initial Stability': -4553.1581838164075, 'Sustained stability': -5810.105333151128}\n",
            "Value Iteration 106: Max Change: 0.007083573314048408, Value Function: {'Initial Increase': -4043.543621660144, 'Sustained Increase': -4259.522957124353, 'Initial Decrease': -4043.543621660144, 'Sustained Decrease': -4259.522957124353, 'Initial Stability': -4553.165267389721, 'Sustained stability': -5810.1124167244425}\n",
            "Value Iteration 107: Max Change: 0.00637521598218882, Value Function: {'Initial Increase': -4043.549996876126, 'Sustained Increase': -4259.529332340335, 'Initial Decrease': -4043.549996876126, 'Sustained Decrease': -4259.529332340335, 'Initial Stability': -4553.171642605703, 'Sustained stability': -5810.118791940425}\n",
            "Value Iteration 108: Max Change: 0.0057376943850613316, Value Function: {'Initial Increase': -4043.5557345705097, 'Sustained Increase': -4259.535070034719, 'Initial Decrease': -4043.5557345705097, 'Sustained Decrease': -4259.535070034719, 'Initial Stability': -4553.177380300087, 'Sustained stability': -5810.12452963481}\n",
            "Value Iteration 109: Max Change: 0.0051639249463732995, Value Function: {'Initial Increase': -4043.560898495456, 'Sustained Increase': -4259.540233959664, 'Initial Decrease': -4043.560898495456, 'Sustained Decrease': -4259.540233959664, 'Initial Stability': -4553.182544225033, 'Sustained stability': -5810.129693559756}\n",
            "Value Iteration 110: Max Change: 0.00464753245250904, Value Function: {'Initial Increase': -4043.5655460279067, 'Sustained Increase': -4259.5448814921165, 'Initial Decrease': -4043.5655460279067, 'Sustained Decrease': -4259.5448814921165, 'Initial Stability': -4553.187191757485, 'Sustained stability': -5810.134341092207}\n",
            "Value Iteration 111: Max Change: 0.00418277920653054, Value Function: {'Initial Increase': -4043.5697288071133, 'Sustained Increase': -4259.549064271323, 'Initial Decrease': -4043.5697288071133, 'Sustained Decrease': -4259.549064271323, 'Initial Stability': -4553.191374536691, 'Sustained stability': -5810.138523871413}\n",
            "Value Iteration 112: Max Change: 0.003764501286241284, Value Function: {'Initial Increase': -4043.5734933083995, 'Sustained Increase': -4259.552828772608, 'Initial Decrease': -4043.5734933083995, 'Sustained Decrease': -4259.552828772608, 'Initial Stability': -4553.195139037977, 'Sustained stability': -5810.142288372699}\n",
            "Value Iteration 113: Max Change: 0.00338805115688956, Value Function: {'Initial Increase': -4043.5768813595564, 'Sustained Increase': -4259.556216823765, 'Initial Decrease': -4043.5768813595564, 'Sustained Decrease': -4259.556216823765, 'Initial Stability': -4553.198527089134, 'Sustained stability': -5810.145676423856}\n",
            "Value Iteration 114: Max Change: 0.003049246041882725, Value Function: {'Initial Increase': -4043.579930605597, 'Sustained Increase': -4259.559266069807, 'Initial Decrease': -4043.579930605597, 'Sustained Decrease': -4259.559266069807, 'Initial Stability': -4553.201576335176, 'Sustained stability': -5810.148725669897}\n",
            "Value Iteration 115: Max Change: 0.0027443214380582504, Value Function: {'Initial Increase': -4043.582674927035, 'Sustained Increase': -4259.562010391243, 'Initial Decrease': -4043.582674927035, 'Sustained Decrease': -4259.562010391243, 'Initial Stability': -4553.204320656611, 'Sustained stability': -5810.151469991333}\n",
            "Value Iteration 116: Max Change: 0.002469889293934102, Value Function: {'Initial Increase': -4043.5851448163276, 'Sustained Increase': -4259.564480280536, 'Initial Decrease': -4043.5851448163276, 'Sustained Decrease': -4259.564480280536, 'Initial Stability': -4553.206790545905, 'Sustained stability': -5810.153939880626}\n",
            "Value Iteration 117: Max Change: 0.002222900364358793, Value Function: {'Initial Increase': -4043.5873677166915, 'Sustained Increase': -4259.566703180901, 'Initial Decrease': -4043.5873677166915, 'Sustained Decrease': -4259.566703180901, 'Initial Stability': -4553.209013446269, 'Sustained stability': -5810.156162780991}\n",
            "Value Iteration 118: Max Change: 0.002000610328195762, Value Function: {'Initial Increase': -4043.5893683270197, 'Sustained Increase': -4259.5687037912285, 'Initial Decrease': -4043.5893683270197, 'Sustained Decrease': -4259.5687037912285, 'Initial Stability': -4553.2110140565965, 'Sustained stability': -5810.158163391318}\n",
            "Value Iteration 119: Max Change: 0.001800549295694509, Value Function: {'Initial Increase': -4043.5911688763144, 'Sustained Increase': -4259.570504340523, 'Initial Decrease': -4043.5911688763144, 'Sustained Decrease': -4259.570504340523, 'Initial Stability': -4553.212814605892, 'Sustained stability': -5810.159963940614}\n",
            "Value Iteration 120: Max Change: 0.0016204943658522097, Value Function: {'Initial Increase': -4043.59278937068, 'Sustained Increase': -4259.572124834889, 'Initial Decrease': -4043.59278937068, 'Sustained Decrease': -4259.572124834889, 'Initial Stability': -4553.214435100257, 'Sustained stability': -5810.161584434979}\n",
            "Value Iteration 121: Max Change: 0.0014584449290850898, Value Function: {'Initial Increase': -4043.5942478156085, 'Sustained Increase': -4259.573583279817, 'Initial Decrease': -4043.5942478156085, 'Sustained Decrease': -4259.573583279817, 'Initial Stability': -4553.215893545186, 'Sustained stability': -5810.163042879907}\n",
            "Value Iteration 122: Max Change: 0.001312600435994682, Value Function: {'Initial Increase': -4043.5955604160445, 'Sustained Increase': -4259.574895880253, 'Initial Decrease': -4043.5955604160445, 'Sustained Decrease': -4259.574895880253, 'Initial Stability': -4553.217206145622, 'Sustained stability': -5810.164355480343}\n",
            "Value Iteration 123: Max Change: 0.0011813403925771127, Value Function: {'Initial Increase': -4043.596741756437, 'Sustained Increase': -4259.576077220646, 'Initial Decrease': -4043.596741756437, 'Sustained Decrease': -4259.576077220646, 'Initial Stability': -4553.218387486015, 'Sustained stability': -5810.165536820736}\n",
            "Value Iteration 124: Max Change: 0.0010632063531375024, Value Function: {'Initial Increase': -4043.59780496279, 'Sustained Increase': -4259.577140426999, 'Initial Decrease': -4043.59780496279, 'Sustained Decrease': -4259.577140426999, 'Initial Stability': -4553.219450692367, 'Sustained stability': -5810.166600027089}\n",
            "Value Iteration 125: Max Change: 0.000956885718551348, Value Function: {'Initial Increase': -4043.5987618485074, 'Sustained Increase': -4259.578097312717, 'Initial Decrease': -4043.5987618485074, 'Sustained Decrease': -4259.578097312717, 'Initial Stability': -4553.2204075780855, 'Sustained stability': -5810.167556912807}\n",
            "Value Iteration: Value function:  {'Initial Increase': -4043.59780496279, 'Sustained Increase': -4259.577140426999, 'Initial Decrease': -4043.59780496279, 'Sustained Decrease': -4259.577140426999, 'Initial Stability': -4553.219450692367, 'Sustained stability': -5810.166600027089}\n",
            "Value Iteration: Policy:  {'Initial Increase': 'Buy', 'Sustained Increase': 'Sell', 'Initial Decrease': 'Sell', 'Sustained Decrease': 'Buy', 'Initial Stability': 'Buy', 'Sustained stability': 'Buy'}\n"
          ]
        }
      ],
      "source": [
        "v2, p2 = value_iteration(fin_MDP.states, actions2, transition_probs2, rewards2, debug_print=True)\n",
        "print(\"Value Iteration: Value function: \", v2)\n",
        "print(\"Value Iteration: Policy: \", p2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61cdeb71",
      "metadata": {},
      "source": [
        "### Solve MDP using Policy Iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 672,
      "id": "7laXWu9-w3YI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7laXWu9-w3YI",
        "outputId": "b1842e89-ce65-42c0-f1d8-39c301ac53dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy Iteration 1: Value Function after Policy Evaluation: {'Initial Increase': -4064.3908648060337, 'Sustained Increase': -4283.5698372290535, 'Initial Decrease': -4068.1774396769456, 'Sustained Decrease': -4277.02938972475, 'Initial Stability': -4572.178748242382, 'Sustained stability': -5824.75066528691}\n",
            "Policy Iteration 1: Policy after Improvement: {'Initial Increase': 'Buy', 'Sustained Increase': 'Sell', 'Initial Decrease': 'Sell', 'Sustained Decrease': 'Buy', 'Initial Stability': 'Buy', 'Sustained stability': 'Buy'} \n",
            "\n",
            "Policy Iteration 2: Value Function after Policy Evaluation: {'Initial Increase': -4043.59780496279, 'Sustained Increase': -4259.577140426999, 'Initial Decrease': -4043.59780496279, 'Sustained Decrease': -4259.577140426999, 'Initial Stability': -4553.219450692367, 'Sustained stability': -5810.166600027089}\n",
            "Policy Iteration 2: Policy after Improvement: {'Initial Increase': 'Buy', 'Sustained Increase': 'Sell', 'Initial Decrease': 'Sell', 'Sustained Decrease': 'Buy', 'Initial Stability': 'Buy', 'Sustained stability': 'Buy'} \n",
            "\n",
            "\n",
            " Policy Iteration: Value function:  {'Initial Increase': -4043.59780496279, 'Sustained Increase': -4259.577140426999, 'Initial Decrease': -4043.59780496279, 'Sustained Decrease': -4259.577140426999, 'Initial Stability': -4553.219450692367, 'Sustained stability': -5810.166600027089}\n",
            "Policy Iteration: Policy:  {'Initial Increase': 'Buy', 'Sustained Increase': 'Sell', 'Initial Decrease': 'Sell', 'Sustained Decrease': 'Buy', 'Initial Stability': 'Buy', 'Sustained stability': 'Buy'}\n"
          ]
        }
      ],
      "source": [
        "# Execute Policy Iteration Algorithm\n",
        "V2, policy2 = policy_iteration(fin_MDP.states, actions2, transition_probs2, rewards2, debug_print=True)\n",
        "\n",
        "# Display outcomes\n",
        "print(\"\\n Policy Iteration: Value function: \", V2)\n",
        "print(\"Policy Iteration: Policy: \", policy2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Qi92nPqtD88",
      "metadata": {
        "id": "4Qi92nPqtD88"
      },
      "source": [
        "### Compare both algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 665,
      "id": "KSz1pDvLtGT0",
      "metadata": {
        "id": "KSz1pDvLtGT0"
      },
      "outputs": [],
      "source": [
        "def simulate_for_average_reward(states, transition_probs, rewards, start_state, policy, max_steps=100, num_episodes=1000):\n",
        "    \"\"\"\n",
        "    Simulate the environment to compute the average reward over a number of episodes.\n",
        "\n",
        "    Parameters:\n",
        "    - states: List of states\n",
        "    - transition_probs: Dictionary of transition probabilities\n",
        "    - rewards: Dictionary of rewards for state-action pairs\n",
        "    - start_state (Any): The initial state from which the simulation begins.\n",
        "    - policy (dict): A mapping from states to actions representing the agent's policy.\n",
        "    - max_steps (int, optional): Maximum number of steps for each episode. Defaults to 100.\n",
        "    - num_episodes (int, optional): Number of episodes to simulate. Defaults to 1000.\n",
        "\n",
        "    Returns:\n",
        "    - float: The average reward accumulated per episode over the specified number of episodes.\n",
        "    \"\"\"\n",
        "    total_rewards = 0\n",
        "    for _ in range(num_episodes):\n",
        "        current_state = start_state\n",
        "        for _ in range(max_steps):\n",
        "            action = policy[current_state]\n",
        "\n",
        "            # Sample the next state based on the transition probabilities\n",
        "            if np.sum(np.array(list(transition_probs[current_state][action].values())))!=0:\n",
        "              # Convert transition_probs[current_state][action].values() to a list\n",
        "              prob_list = list(transition_probs[current_state][action].values())\n",
        "              # Normalize the prob_list to ensure it sums to 1\n",
        "              prob_list = [p / sum(prob_list) for p in prob_list]\n",
        "              next_state = np.random.choice(states, p=prob_list)\n",
        "\n",
        "            # Access the reward using next_state\n",
        "            reward = rewards[current_state][action][next_state]\n",
        "            total_rewards += reward\n",
        "            current_state = next_state\n",
        "    return total_rewards / num_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 666,
      "id": "ZgbQ65UytYWz",
      "metadata": {
        "id": "ZgbQ65UytYWz"
      },
      "outputs": [],
      "source": [
        "def compare_algorithms(states, actions, transition_probs, rewards, start_state, gamma=0.9, theta=1e-3):\n",
        "    \"\"\"\n",
        "    Compare the performance of Policy Iteration (PI) and Value Iteration (VI) on the given environment.\n",
        "\n",
        "    Parameters:\n",
        "    - states: List of states\n",
        "    - transition_probs: Dictionary of transition probabilities\n",
        "    - rewards: Dictionary of rewards for state-action pairs\n",
        "    - start_state (Any): The initial state from which the simulations begin.\n",
        "    - gamma (float, optional): The discount factor used in both algorithms. Defaults to 0.9.\n",
        "    - eps (float, optional): The threshold for determining convergence in both algorithms. Defaults to 1e-3.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing performance metrics and results for both algorithms:\n",
        "      * \"Value Iteration\" and \"Policy Iteration\" each contain a sub-dictionary with:\n",
        "        - \"Convergence Time\": Time taken for the algorithm to converge.\n",
        "        - \"Value Function\": The final value function determined by the algorithm.\n",
        "        - \"Policy\": The optimal policy determined by the algorithm.\n",
        "        - \"Average Reward\": The average reward when following the computed policy from the start state.\n",
        "      * \"Value Function Difference\": The sum of absolute differences between value functions of PI and VI.\n",
        "      * \"Policy Difference\": The number of states where PI and VI policies differ.\n",
        "\n",
        "    Notes:\n",
        "    This function first runs Policy Iteration and then Value Iteration on the environment. After both algorithms\n",
        "    have been executed, it compares their value functions and policies, calculates the average reward for both\n",
        "    policies, and compiles these results into a dictionary.\n",
        "    \"\"\"\n",
        "    # Policy Iteration\n",
        "    start_time = time.time()\n",
        "    PI_values, PI_policy = policy_iteration(states, actions, transition_probs, rewards, gamma, theta, debug_print=False)\n",
        "    PI_time = time.time() - start_time\n",
        "    PI_avg_reward = simulate_for_average_reward(states, transition_probs, rewards, start_state, PI_policy)\n",
        "\n",
        "    # Value Iteration\n",
        "    start_time = time.time()\n",
        "    VI_values, VI_policy = value_iteration(states, actions, transition_probs, rewards, gamma, theta, debug_print=False)\n",
        "    VI_time = time.time() - start_time\n",
        "    VI_avg_reward = simulate_for_average_reward(states, transition_probs, rewards, start_state, VI_policy)\n",
        "\n",
        "    # Metrics\n",
        "    value_diff = sum([abs(VI_values[state] - PI_values[state]) for state in states])\n",
        "    policy_diff = sum([1 if VI_policy[state] != PI_policy[state] else 0 for state in states])\n",
        "\n",
        "    return {\n",
        "        \"Value Iteration\": {\n",
        "            \"Convergence Time\": VI_time,\n",
        "            \"Value Function\": VI_values,\n",
        "            \"Policy\": VI_policy,\n",
        "            \"Average Reward\": VI_avg_reward\n",
        "        },\n",
        "        \"Policy Iteration\": {\n",
        "            \"Convergence Time\": PI_time,\n",
        "            \"Value Function\": PI_values,\n",
        "            \"Policy\": PI_policy,\n",
        "            \"Average Reward\": PI_avg_reward\n",
        "        },\n",
        "        \"Value Function Difference (Sum of Absolute Differences)\": value_diff,\n",
        "        \"Policy Difference (Number of Different Actions)\": policy_diff\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 673,
      "id": "HAnEhrRFw3ag",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAnEhrRFw3ag",
        "outputId": "a6a14c1c-f0f4-4975-f502-233402467fa1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Value Iteration': {'Convergence Time': 0.0010499954223632812,\n",
              "  'Value Function': {'Initial Increase': -1422.4351484820472,\n",
              "   'Sustained Increase': -1629.0692030445266,\n",
              "   'Initial Decrease': -1422.4351484820472,\n",
              "   'Sustained Decrease': -1629.0692030445266,\n",
              "   'Initial Stability': -1866.5969546904364,\n",
              "   'Sustained stability': -2780.911159768478},\n",
              "  'Policy': {'Initial Increase': 'Buy',\n",
              "   'Sustained Increase': 'Sell',\n",
              "   'Initial Decrease': 'Sell',\n",
              "   'Sustained Decrease': 'Buy',\n",
              "   'Initial Stability': 'Buy',\n",
              "   'Sustained stability': 'Buy'},\n",
              "  'Average Reward': -44891.77},\n",
              " 'Policy Iteration': {'Convergence Time': 0.0008389949798583984,\n",
              "  'Value Function': {'Initial Increase': -1422.4351484820472,\n",
              "   'Sustained Increase': -1629.0692030445266,\n",
              "   'Initial Decrease': -1422.4351484820472,\n",
              "   'Sustained Decrease': -1629.0692030445266,\n",
              "   'Initial Stability': -1866.5969546904364,\n",
              "   'Sustained stability': -2780.911159768478},\n",
              "  'Policy': {'Initial Increase': 'Buy',\n",
              "   'Sustained Increase': 'Sell',\n",
              "   'Initial Decrease': 'Sell',\n",
              "   'Sustained Decrease': 'Buy',\n",
              "   'Initial Stability': 'Buy',\n",
              "   'Sustained stability': 'Buy'},\n",
              "  'Average Reward': -45296.92},\n",
              " 'Value Function Difference (Sum of Absolute Differences)': 0.0,\n",
              " 'Policy Difference (Number of Different Actions)': 0}"
            ]
          },
          "execution_count": 673,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_algorithms(fin_MDP.states, actions2, transition_probs2, rewards2, 'Initial Stability',gamma=0.75)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
